{'seed': None, 'notes': None, 'non_verbose': 0, 'disable_log': 0, 'validation': 0, 'ignore_other_metrics': 0, 'debug_mode': 0, 'nowand': 0, 'wandb_entity': 'fneubuerger', 'wandb_project': 'mammoth', 'dataset': 'seq-cifar10', 'model': 'er', 'lr': 0.0001, 'optim_wd': 0.0, 'optim_mom': 0.0, 'optim_nesterov': 0, 'n_epochs': 20, 'batch_size': 32, 'distributed': 'no', 'buffer_size': 500, 'minibatch_size': 32, 'conf_jobnum': '43f56c14-252e-4270-9794-9a8075d11567', 'conf_timestamp': '2023-04-17 09:10:31.723664', 'conf_host': 'DSFHSWF-PowerEdge-R7525', 'wandb_url': 'https://wandb.ai/fneubuerger/mammoth/runs/vqm1uq6t', 'accmean_task1': 86.45, 'accmean_task2': 68.525, 'accmean_task3': 48.883333333333326, 'accmean_task4': 41.9125, 'accmean_task5': 37.89, 'accuracy_1_task1': 86.45, 'accuracy_1_task2': 67.75, 'accuracy_2_task2': 69.3, 'accuracy_1_task3': 69.35, 'accuracy_2_task3': 1.7500000000000002, 'accuracy_3_task3': 75.55, 'accuracy_1_task4': 63.2, 'accuracy_2_task4': 12.1, 'accuracy_3_task4': 11.200000000000001, 'accuracy_4_task4': 81.15, 'accuracy_1_task5': 16.55, 'accuracy_2_task5': 23.95, 'accuracy_3_task5': 30.15, 'accuracy_4_task5': 37.75, 'accuracy_5_task5': 81.05, 'forward_transfer': -12.212499999999999, 'backward_transfer': -51.0125, 'forgetting': 51.0125}
{'seed': None, 'notes': None, 'non_verbose': 0, 'disable_log': 0, 'validation': 0, 'ignore_other_metrics': 0, 'debug_mode': 0, 'nowand': 0, 'wandb_entity': 'fneubuerger', 'wandb_project': 'mammoth', 'dataset': 'seq-cifar10', 'model': 'er', 'lr': 0.0001, 'optim_wd': 0.0, 'optim_mom': 0.0, 'optim_nesterov': 0, 'n_epochs': 50, 'batch_size': 32, 'distributed': 'no', 'buffer_size': 200, 'minibatch_size': 32, 'conf_jobnum': '6269ebc4-bbdc-467e-8c3a-dafa059a961a', 'conf_timestamp': '2023-11-16 09:45:05.229611', 'conf_host': 'DSFHSWF-PowerEdge-R7525', 'wandb_url': 'https://wandb.ai/fneubuerger/mammoth/runs/21imywqe', 'accmean_task1': 91.60000000000001, 'accmean_task2': 60.75, 'accmean_task3': 41.61666666666667, 'accmean_task4': 32.8125, 'accmean_task5': 33.17, 'accuracy_1_task1': 91.60000000000001, 'accuracy_1_task2': 43.15, 'accuracy_2_task2': 78.35, 'accuracy_1_task3': 40.050000000000004, 'accuracy_2_task3': 7.8, 'accuracy_3_task3': 77.0, 'accuracy_1_task4': 23.7, 'accuracy_2_task4': 6.9, 'accuracy_3_task4': 12.65, 'accuracy_4_task4': 88.0, 'accuracy_1_task5': 2.1, 'accuracy_2_task5': 8.4, 'accuracy_3_task5': 13.450000000000001, 'accuracy_4_task5': 53.949999999999996, 'accuracy_5_task5': 87.94999999999999, 'forward_transfer': -0.0125, 'backward_transfer': -64.2625, 'forgetting': 64.2625}
